# Tensorflow

v1

## tf神经网络结构

1. 定义tf变量—— tf.Variable
2. 定义loss
3. 定义反向传播误差
4. 开始训练

## Variable

```python
a = tf.Variable(0)
b = f.constant(1)
c = tf.add(a, b)
update = tf.assign(a, c)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for _ in range(3):
        sess.run(update)
        print(sess.run(state))
```

## Placeholder 传入值

```python
input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)

ouput = tf.multiply(input1, input2)

with tf.Session() as sess:
    print(sess.run(ouput, feed_dict={input1: [7.], input2: [2.]}))
```

## 激励函数(Activation Function)

将线性函数$y=Wx$处理$y=AF(Wx)$为非线性函数

![ ](https://morvanzhou.github.io/static/results/ML-intro/active3.png)

可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去.

## 添加一个层

```python
def add_layer(inputs, in_size, out_size, activation_function=None):
    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    return outputs
```

## 构建基础的神经网络

## 加速训练——使用优化器（optimizer）

1. SGD——将数据分批放入NN中训练
2. 优化更新weight的函数

    1. Momentum
    2. AdaGrad
    3. RMSProp
    4. Adam

## tensorboard 可视化

## 分类器 

loss函数使用cross entropy

## 过拟合问题与解决思路

![过拟合](https://morvanzhou.github.io/static/results/ML-intro/overfitting4.png)

1. 增加数据量
2. 使用正规化

    1. L1
    2. L2
    3. Dropout

## CNN

## RNN

当前时刻的预测依赖于当前的输入和之前的状态

## LSTM RNN

RNN会出现梯度消失和梯度爆炸的问题，LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制), 来解决这两个问题
![ ](https://morvanzhou.github.io/static/results/ML-intro/lstm5.png)

# Keras